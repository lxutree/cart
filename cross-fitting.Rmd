---
title: "Cross-fitting"
author: "Liang"
date: "04/04/2022"
output: html_document
---
##

### Motivation

Most methods for the average causal effect require estimation of certain nuisance functions:

- probability of treatment conditional on covariates

- probability of outcome conditional on treatment and covariates


Doubly-robust estimators, e.g., augmented inverse probability weighted estimator (AIPW) and targeted maximum likelihood estimator (TMLE):

- two chances to guess the correct nuisance model

- allow for slower converging models

- limitation: nuisance models need to be simple (Donsker class). Overly narrow confidence intervals if not in this class.

**Cross-fitting allows for complex nuisance models.** Cross-fitted and doubly-robust models are compatible with a wide range of machine learning methods. 

### Double cross-fit

step 0: dataset with outcome, treatment and minimal sufficient adjustment set of confounders

step 1: partition dataset into $p$ splits

step 2: fit two nuisance models (for treatment and outcome) in each split

step 3: generate predictions using models estimated from **discordant** datasets (each nuisance model from a different split) 

step 4: point ACE ($ACE_p$) estimate obtained from the average of all predictions across splits. 

step 5: repeat above steps for a different way of splitting data. 

step 6: overall ACE ($\widetilde{ACE}$) calculated as the median of point estimates across all splits. Variance of $\widetilde{ACE}$ is as follows:

$$Var (\widetilde{ACE}) = \text{median}(Var(ACE_p) + (ACE_p - \widetilde{ACE})^2)$$
Limitations:

- long run-time due to partitioning and repetitions

- finite sample problem - especially with many splits and k-fold super learners.


## Cross-validation: what does it estimate and how well does it do it?

### Main Points:

- MSE of CV point estimate of prediction error is lower for Err and Err_x than Err_xy and has higher coverage for Err and Err_x than Err_xy

![](crossfit/fig3.png)
- Both Err_x and Err_xy have mean Err, and variance of Err_x is smaller than that of Err_xy 
![](crossfit/eq6.png)

- CV point estimate of prediction error is really an estimate of the average prediction error 

- Nested CV estimate of MSE:

![](crossfit/nested.png)

### Methods

Target parameter: ATE

ATE estimators:

- IPTW

- AIPW (with or without cross-fitting)

- TMLE (with or without cross-fitting)


Models for nuisance parameters (outcome & PS models):

- Cross-validated ensemble learner with smooth methods (e.g. GLM, LASSO)

- Cross-validated ensemble learner with non-smooth methods (e.g. boosting, random forest)

### Dataset

Growing Up in Singapore Towards healthy Outcomes:

- n=1178

- 331 covariates

### Simulation scenarios:
 
##### Scenario A: true model more complex than fitted model

True models:

- Outcome model: $Y \sim A + (X_1 + X_2 + X_5 + X_{18} + X_{217})^2 + \sum_{i=1}^{40} X_i$

- PS model: $\text{logit}(P(A=1|X)) \sim (X_1 + X_2 + X_5 + X_{18} + X_{217})^2 + \sum_{i=1}^{40} X_i$

Misspecified first order terms ('A.less.1st'):

- Outcome model: $Y \sim A + (X_1 + X_2 + X_5 + X_{18} + X_{217})^2 + \sum_{i\in A} X_i$

- PS model: $\text{logit}(P(A=1|X)) \sim (X_1 + X_2 + X_5 + X_{18} + X_{217})^2 + \sum_{i\in A} X_i$

Misspecified interactions terms ('A.no.int'):

- Outcome model: $Y \sim A + X_1 + X_2 + X_5 + X_{18} + X_{217} + \sum_{i=1}^{40} X_i$

- PS model: $\text{logit}(P(A=1|X)) \sim X_1 + X_2 + X_5 + X_{18} + X_{217} + \sum_{i=1}^{40} X_i$

##### Scenario B: true model simpler than fitted model

True models:

- Outcome model: $Y \sim A + (X_1 + X_2 + X_5 + X_{18} + X_{217})^2 + \sum_{i\in A} X_i$

- PS model: $\text{logit}(P(A=1|X)) \sim (X_1 + X_2 + X_5 + X_{18} + X_{217})^2 + \sum_{i\in A} X_i$

Fitted models:

- Outcome model: $Y \sim A + (X_1 + X_2 + X_5 + X_{18} + X_{217})^2 + \sum_{i=1}^{40} X_i$

- PS model: $\text{logit}(P(A=1|X)) \sim (X_1 + X_2 + X_5 + X_{18} + X_{217})^2 + \sum_{i=1}^{40} X_i$

##### Scenario C: Interaction between treatment and covariates

True models:

- Outcome model: $Y \sim  (A +X_1 + X_2 + X_5 + X_{18} + X_{217})^2$

- PS model: $\text{logit}(P(A=1|X)) \sim  (X_1 + X_2 + X_5 + X_{18} + X_{217})^2$

Partially misspecified (C.part):

- Outcome model: $Y \sim A + A:X_1 + A:X_{217} + (X_1 + X_2 + X_5 + X_{18} + X_{217})^2$

- PS model: $\text{logit}(P(A=1|X)) \sim (X_1 + X_2 + X_5 + X_{18} + X_{217})^2$

No interaction terms (C.bad):

- Outcome model: $Y \sim A + (X_1 + X_2 + X_5 + X_{18} + X_{217})^2$

- PS model: $\text{logit}(P(A=1|X)) \sim (X_1 + X_2 + X_5 + X_{18} + X_{217})^2$

### Results

![](crossfit/table2.png)


- poor performance across the board

- TMLE/AIPW with non-smooth learners: poor coverage due to small SE estimates

- AIPW with smooth learners: minimal bias but sub-optimal coverage

- Omitting interactions had less of an impact than removing first order terms

![](crossfit/table3.png)


- Combining smooth and non-smooth learners produced results that are the average of those with only smooth learners or non-smooth leaners

- when all 331 covariates included, double-crossfitting is preferred (coverage 85% and 95% for TMLE and AIPW)

![](crossfit/table4.png)

- good coverage with smooth learners for all estimators

- IPW: poor bias and overly conservative coverage due to large SEs


![](crossfit/table5.png)

- double-crossfitting: overly conservative coverage

- non-crossfit estimators with smooth learners ahd better coverage 