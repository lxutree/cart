}
return(pred = pred_list[[iter+1]])
}
weight = rep(1/NN, nrow(data))
weight
tree = classtree(data = data, min.obs = min.obs, resp = resp, depth = depth)
pred = predtree(newdata = data, resp = resp, res = tree$output, data = data)[, resp]
missclass = sum(pred != data[,resp])
error = sum(pred != data[,resp]) / NN
stageValue = log((1 - error) / error)
weight = weight * exp(ifelse(test = pred != data[,resp], -1, 1) * stageValue)
weight
sample(1:NN, NN, replace = TRUE, prob = weight)
newdata = data[sample(1:NN, NN, replace = TRUE, prob = weight),]
newdata
data = data[sample(1:NN, NN, replace = TRUE, prob = weight),]
tree = classtree(data = data, min.obs = min.obs, resp = resp, depth = depth)
pred = predtree(newdata = data, resp = resp, res = tree$output, data = data)[, resp]
missclass = sum(pred != data[,resp])
error = sum(pred != data[,resp]) / NN
stageValue = log((1 - error) / error)
weight = weight * exp(ifelse(test = pred != data[,resp], -1, 1) * stageValue)
data = data[sample(1:NN, NN, replace = TRUE, prob = weight),]
weight
data[sample(1:NN, NN, replace = TRUE, prob = weight),]
data[sample(1:NN, NN, replace = TRUE, prob = weight),]
missclass
error
install.packages(gbm)
install.packages("gbm")
library (gbm)
boost.boston <- gbm(Kyphosis ∼ ., data = kyphosis,
distribution = " gaussian ", n. trees = 100 ,
interaction.depth = 4)
boost.boston <- gbm(Kyphosis ∼ ., data = kyphosis, distribution = " gaussian ", n. trees = 100 , interaction.depth = 4)
boost.boston <- gbm(Kyphosis ∼ ., data = kyphosis, distribution = "gaussian", n.trees = 100 , interaction.depth = 4)
boost.boston
summary(boost.boston)
boost.boston <- gbm(Kyphosis ∼ ., data = kyphosis, distribution = "gaussian", n.trees = 5000 , interaction.depth = 4)
summary(boost.boston)
Boston
boost(data = kyphosis, resp = "Kyphosis", depth = 4, iter = 200, min.obs = 2)
boost = function(data, resp, depth, iter, min.obs, rate, ...){
# depth = maximum of terminal nodes allowed. This is equal to the number of splits + 1
# iter = number of iterations
feat = names(data)[names(data)!=resp]
# for continuous response:
if (!is.factor(data[,resp])){
pred_list = tree_list = importance_list = resid_list = list()
pred_list[[1]] = rep(mean(data[, resp]), nrow(data))
resid_list[[1]] = data[, resp] - pred_list[[1]]
for (i in 1:iter){
tree = regtree(data = data.frame(data[,feat], resid = resid_list[[i]]), min.obs = min.obs, resp = "resid", depth = depth)
tree_list[[i]] = tree$output
pred_resid = predtree(newdata = data, resp = resp, res = tree$output, data = data)[,resp]
pred_list[[i+1]] =  pred_list[[i]] + pred_resid*rate
resid_list[[i + 1]] = data[, resp] - pred_list[[i+1]]
importance_list[[i]] = tree$var_rank
}
importance = apply(do.call(cbind, importance_list), MARGIN = 1, mean)
return(list(pred = pred_list[[iter+1]], importance = importance, trees = tree_list, resp = resp, data = data, rate = rate))
}
}
boost(data = kyphosis, resp = "Kyphosis", depth = 4, iter = 200, min.obs = 2)
test=boost(data = kyphosis, resp = "Kyphosis", depth = 4, iter = 200, min.obs = 2)
test
Boston
library(MASS)
Boston
test=boost(data = Boston, resp = "medv", depth = 4, iter = 200, min.obs = 2)
test=boost(data = Boston, resp = "medv", depth = 4, iter = 200, min.obs = 20)
test=boost(data = Boston, resp = "medv", depth = 20, iter = 200, min.obs = 20)
data = Boston
Boston$medv
min.obs = 20
# minimum size of leaves?
leafsize = min.obs/3 # same as the default in rpart
# data.frame to store results:
output = data.frame(status = "split", count = nrow(data), "split rule" = "root", iter = 0, mean = mean(data[, resp]), stringsAsFactors = FALSE)
data[, resp]
resp
resp = "medv"
# minimum size of leaves?
leafsize = min.obs/3 # same as the default in rpart
# data.frame to store results:
output = data.frame(status = "split", count = nrow(data), "split rule" = "root", iter = 0, mean = mean(data[, resp]), stringsAsFactors = FALSE)
output
# list of observations in each node/leaf:
data.list = list()
# data at the root:
data.list[[1]] = data
# indicator for whether the tree keeps growing:
stopsplit = FALSE
iter = 1
# list of features:
if(is.null(feat)) feat = allfeat = names(data)[names(data)!=resp]
if(is.null(nfeat)) nfeat = round(sqrt(length(feat)))
# vector of features/gini index reduction for each split:
feat_vec = c()
diff_vec = c()
# list of splits to be done:
split.queue = which(output$status == "split")
for (j in split.queue[1]) {
# empty vector
weighted.sd = c()
error = c()
split_val = c()
data.temp = data.list[[j]]
# for random forrest
if(!is.null(type)){if(type == "rf") feat = sample(allfeat, size = nfeat, replace = FALSE)}
for (i in 1:length(feat)){
data_sub = data.frame(var = data.temp[, feat[i]], resp = data.temp[, resp])
# gini index of parent node
rss_parent = sum((data_sub$resp - mean(data_sub$resp))^2)
# calculating sse for categorical feature:
if( is.factor(data_sub$var) ) {
data.split = split(data_sub, data_sub$var)
error[i] = sum(sapply(data.split, function(x){
sum( (x$resp - mean(x$resp)) ^ 2 )
}))
# checking size of children nodes; if less than specified, the split is not considered
count_min = min(sapply(data.split, nrow))
if( count_min < leafsize) error[i] = NA
} else {
# calculating gini index for continuous feature:
splits_sort = sort(unique(data_sub$var)) # all possible splits
sse <- c()
# calculating sse for all possible splits
for( k in 1:length(splits_sort)){
sse[k] = sum( (data_sub$resp[data_sub$var < splits_sort[k]] - mean(data_sub$resp[data_sub$var < splits_sort[k]]) )^2 ) + sum( (data_sub$resp[data_sub$var >= splits_sort[k]] - mean(data_sub$resp[data_sub$var >= splits_sort[k]]) )^2 )
# checking size of children nodes; if less than specified, the split is not considered
count_min = min(length(data_sub$resp[data_sub$var < splits_sort[k]]), length(data_sub$resp[data_sub$var >= splits_sort[k]]))
if(count_min < round(leafsize)) sse[k] = NA
}
# clean up for when none of the splits is valid:
if(all(is.na(sse))) {error[i] = NA; split_val[i] = NA} else { error[i] = min(sse, na.rm = TRUE); split_val[i] = splits_sort[which.min(sse)]}
}
}
# characteristics of the current split
splitvar = feat[which.min(error)] # feature leading to the lowest sse
rss_diff = rss_parent - min(error, na.rm = TRUE) # difference in rss
feat_vec = c(feat_vec, splitvar) # recorded in vector
diff_vec = c(diff_vec, rss_diff) # recorded in vector
# creating children nodes:
if( is.factor(data.temp[[splitvar]])) {
# for categorical feature:
data.next = split(data.temp, data.temp[ , splitvar])
} else {
# for continuous feature:
index = which(sort(unique(data.temp[[splitvar]])) == value)
# taking the middle point of unique values as the splitting point to be consistent with 'rpart':
value = (sort(unique(data.temp[[splitvar]]))[index] + sort(unique(data.temp[[splitvar]]))[index-1])/2
data.next = list()
data.next[[1]] = data.temp[which(data.temp[[splitvar]] < value), ]
data.next[[2]] = data.temp[which(data.temp[[splitvar]] > value), ]
}
# Stopping criteria:
# - less than 3 observations
# - all observations have the same label
status = sapply(data.next, function(x){
if (ncol(x) > 2) {
if (nrow(x) < min.obs | nrow( unique(x[, -which(names(x) %in% resp)]) ) == 1) status = "leaf" else status = "split"
} else status = "leaf"
status
})
# change current status from 'split' to 'parent' so it won't be split further:
output$status[j] = "parent"
# record how the split was done:
if( is.factor(data.temp[[splitvar]]) ) {
splitrule = sapply(names(data.next), function(x){paste(splitvar, "=" , x)})
} else {
splitrule = c(paste(splitvar, "<", value),paste(splitvar, ">", value) )
}
# creating outputs
temp.output = data.frame(status = status, count = sapply(data.next, nrow), "split rule" = splitrule, iter = iter, row.names = NULL, mean = sapply(data.next, function(x){mean(x[[resp]])}))
# attach new outputs to existing dataframe
output = rbind(output[1:j,], temp.output, output[-c(1:j), ])
names(data.next) = NULL; data.list = c(data.list[1:j], data.next, data.list[-c(1:j)])
}
feat
feat = NULL
# list of features:
if(is.null(feat)) feat = allfeat = names(data)[names(data)!=resp]
feat
# vector of features/gini index reduction for each split:
feat_vec = c()
diff_vec = c()
# list of splits to be done:
split.queue = which(output$status == "split")
j
# empty vector
weighted.sd = c()
error = c()
split_val = c()
data.temp = data.list[[j]]
# for random forrest
if(!is.null(type)){if(type == "rf") feat = sample(allfeat, size = nfeat, replace = FALSE)}
type
feat
nfeat
nfeat =NULL
type = NULL
# list of observations in each node/leaf:
data.list = list()
# data at the root:
data.list[[1]] = data
# indicator for whether the tree keeps growing:
stopsplit = FALSE
iter = 1
# list of features:
if(is.null(feat)) feat = allfeat = names(data)[names(data)!=resp]
if(is.null(nfeat)) nfeat = round(sqrt(length(feat)))
# vector of features/gini index reduction for each split:
feat_vec = c()
diff_vec = c()
feat
feat = allfeat = names(data)[names(data)!=resp]
# vector of features/gini index reduction for each split:
feat_vec = c()
diff_vec = c()
for (j in split.queue[1]) {
# empty vector
weighted.sd = c()
error = c()
split_val = c()
data.temp = data.list[[j]]
# for random forrest
if(!is.null(type)){if(type == "rf") feat = sample(allfeat, size = nfeat, replace = FALSE)}
for (i in 1:length(feat)){
data_sub = data.frame(var = data.temp[, feat[i]], resp = data.temp[, resp])
# gini index of parent node
rss_parent = sum((data_sub$resp - mean(data_sub$resp))^2)
# calculating sse for categorical feature:
if( is.factor(data_sub$var) ) {
data.split = split(data_sub, data_sub$var)
error[i] = sum(sapply(data.split, function(x){
sum( (x$resp - mean(x$resp)) ^ 2 )
}))
# checking size of children nodes; if less than specified, the split is not considered
count_min = min(sapply(data.split, nrow))
if( count_min < leafsize) error[i] = NA
} else {
# calculating gini index for continuous feature:
splits_sort = sort(unique(data_sub$var)) # all possible splits
sse <- c()
# calculating sse for all possible splits
for( k in 1:length(splits_sort)){
sse[k] = sum( (data_sub$resp[data_sub$var < splits_sort[k]] - mean(data_sub$resp[data_sub$var < splits_sort[k]]) )^2 ) + sum( (data_sub$resp[data_sub$var >= splits_sort[k]] - mean(data_sub$resp[data_sub$var >= splits_sort[k]]) )^2 )
# checking size of children nodes; if less than specified, the split is not considered
count_min = min(length(data_sub$resp[data_sub$var < splits_sort[k]]), length(data_sub$resp[data_sub$var >= splits_sort[k]]))
if(count_min < round(leafsize)) sse[k] = NA
}
# clean up for when none of the splits is valid:
if(all(is.na(sse))) {error[i] = NA; split_val[i] = NA} else { error[i] = min(sse, na.rm = TRUE); split_val[i] = splits_sort[which.min(sse)]}
}
}
# characteristics of the current split
splitvar = feat[which.min(error)] # feature leading to the lowest sse
rss_diff = rss_parent - min(error, na.rm = TRUE) # difference in rss
feat_vec = c(feat_vec, splitvar) # recorded in vector
diff_vec = c(diff_vec, rss_diff) # recorded in vector
# creating children nodes:
if( is.factor(data.temp[[splitvar]])) {
# for categorical feature:
data.next = split(data.temp, data.temp[ , splitvar])
} else {
# for continuous feature:
index = which(sort(unique(data.temp[[splitvar]])) == value)
# taking the middle point of unique values as the splitting point to be consistent with 'rpart':
value = (sort(unique(data.temp[[splitvar]]))[index] + sort(unique(data.temp[[splitvar]]))[index-1])/2
data.next = list()
data.next[[1]] = data.temp[which(data.temp[[splitvar]] < value), ]
data.next[[2]] = data.temp[which(data.temp[[splitvar]] > value), ]
}
# Stopping criteria:
# - less than 3 observations
# - all observations have the same label
status = sapply(data.next, function(x){
if (ncol(x) > 2) {
if (nrow(x) < min.obs | nrow( unique(x[, -which(names(x) %in% resp)]) ) == 1) status = "leaf" else status = "split"
} else status = "leaf"
status
})
# change current status from 'split' to 'parent' so it won't be split further:
output$status[j] = "parent"
# record how the split was done:
if( is.factor(data.temp[[splitvar]]) ) {
splitrule = sapply(names(data.next), function(x){paste(splitvar, "=" , x)})
} else {
splitrule = c(paste(splitvar, "<", value),paste(splitvar, ">", value) )
}
# creating outputs
temp.output = data.frame(status = status, count = sapply(data.next, nrow), "split rule" = splitrule, iter = iter, row.names = NULL, mean = sapply(data.next, function(x){mean(x[[resp]])}))
# attach new outputs to existing dataframe
output = rbind(output[1:j,], temp.output, output[-c(1:j), ])
names(data.next) = NULL; data.list = c(data.list[1:j], data.next, data.list[-c(1:j)])
}
# empty vector
weighted.sd = c()
error = c()
split_val = c()
data.temp = data.list[[j]]
# for random forrest
if(!is.null(type)){if(type == "rf") feat = sample(allfeat, size = nfeat, replace = FALSE)}
feat
for (i in 1:length(feat)){
data_sub = data.frame(var = data.temp[, feat[i]], resp = data.temp[, resp])
# gini index of parent node
rss_parent = sum((data_sub$resp - mean(data_sub$resp))^2)
# calculating sse for categorical feature:
if( is.factor(data_sub$var) ) {
data.split = split(data_sub, data_sub$var)
error[i] = sum(sapply(data.split, function(x){
sum( (x$resp - mean(x$resp)) ^ 2 )
}))
# checking size of children nodes; if less than specified, the split is not considered
count_min = min(sapply(data.split, nrow))
if( count_min < leafsize) error[i] = NA
} else {
# calculating gini index for continuous feature:
splits_sort = sort(unique(data_sub$var)) # all possible splits
sse <- c()
# calculating sse for all possible splits
for( k in 1:length(splits_sort)){
sse[k] = sum( (data_sub$resp[data_sub$var < splits_sort[k]] - mean(data_sub$resp[data_sub$var < splits_sort[k]]) )^2 ) + sum( (data_sub$resp[data_sub$var >= splits_sort[k]] - mean(data_sub$resp[data_sub$var >= splits_sort[k]]) )^2 )
# checking size of children nodes; if less than specified, the split is not considered
count_min = min(length(data_sub$resp[data_sub$var < splits_sort[k]]), length(data_sub$resp[data_sub$var >= splits_sort[k]]))
if(count_min < round(leafsize)) sse[k] = NA
}
# clean up for when none of the splits is valid:
if(all(is.na(sse))) {error[i] = NA; split_val[i] = NA} else { error[i] = min(sse, na.rm = TRUE); split_val[i] = splits_sort[which.min(sse)]}
}
}
splitvar
# characteristics of the current split
splitvar = feat[which.min(error)] # feature leading to the lowest sse
error
# characteristics of the current split
splitvar = feat[which.min(error)] # feature leading to the lowest sse
rss_diff = rss_parent - min(error, na.rm = TRUE) # difference in rss
feat_vec = c(feat_vec, splitvar) # recorded in vector
diff_vec = c(diff_vec, rss_diff) # recorded in vector
# creating children nodes:
if( is.factor(data.temp[[splitvar]])) {
# for categorical feature:
data.next = split(data.temp, data.temp[ , splitvar])
} else {
# for continuous feature:
index = which(sort(unique(data.temp[[splitvar]])) == value)
# taking the middle point of unique values as the splitting point to be consistent with 'rpart':
value = (sort(unique(data.temp[[splitvar]]))[index] + sort(unique(data.temp[[splitvar]]))[index-1])/2
data.next = list()
data.next[[1]] = data.temp[which(data.temp[[splitvar]] < value), ]
data.next[[2]] = data.temp[which(data.temp[[splitvar]] > value), ]
}
sort(unique(data.temp[[splitvar]]))
splitvar
(sort(unique(data.temp[[splitvar]]))[index] + sort(unique(data.temp[[splitvar]]))[index-1])/2
split_val
source('C:/Users/Liang has no secrets/LX_Project/cart/reg.R')
test=boost(data = Boston, resp = "medv", depth = 20, iter = 200, min.obs = 20)
test=boost(data = Boston, resp = "medv", depth = 20, iter = 200, min.obs = 20, rate = 0.1)
test=boost(data = Boston, resp = "medv", depth = 4, iter = 200, min.obs = 20, rate = 0.1)
test=boost(data = Boston, resp = "medv", depth = 4, iter = 20, min.obs = 20, rate = 0.1)
test
test$importance
boost = function(data, resp, depth, iter, min.obs, rate, ...){
# depth = maximum of terminal nodes allowed. This is equal to the number of splits + 1
# iter = number of iterations
feat = names(data)[names(data)!=resp]
# for binary response:
if (is.factor(data[,resp])){
NN = nrow(data)
weight = rep(1/NN, nrow(data))
tree = classtree(data = data, min.obs = min.obs, resp = resp, depth = depth)
pred = predtree(newdata = data, resp = resp, res = tree$output, data = data)[, resp]
missclass = sum(pred != data[,resp])
error = sum(pred != data[,resp]) / NN
stageValue = log((1 - error) / error)
weight = weight * exp(ifelse(test = pred != data[,resp], -1, 1) * stageValue)
data = data[sample(1:NN, NN, replace = TRUE, prob = weight),]
}
# for continuous response:
if (!is.factor(data[,resp])){
pred_list = tree_list = importance_list = resid_list = list()
pred_list[[1]] = rep(mean(data[, resp]), nrow(data))
resid_list[[1]] = data[, resp] - pred_list[[1]]
for (i in 1:iter){
tree = regtree(data = data.frame(data[,feat], resid = resid_list[[i]]), min.obs = min.obs, resp = "resid", depth = depth)
tree_list[[i]] = tree$output
pred_resid = predtree(newdata = data, resp = resp, res = tree$output, data = data)[,resp]
pred_list[[i+1]] =  pred_list[[i]] + pred_resid*rate
resid_list[[i + 1]] = data[, resp] - pred_list[[i+1]]
importance_list[[i]] = tree$var_rank
}
importance = apply(do.call(cbind, importance_list), MARGIN = 1, mean)
return(list(pred = pred_list[[iter+1]], importance = sort(importance, decreasing = TRUE), trees = tree_list, resp = resp, data = data, rate = rate))
}
}
test=boost(data = Boston, resp = "medv", depth = 4, iter = 20, min.obs = 20, rate = 0.1)
test$importance
boost.boston <- gbm(medv ∼ ., data = Boston, distribution = "gaussian", n.trees = 20 , interaction.depth = 4, )
summary(boost.boston)
?gbm
boost.boston <- gbm(medv ∼ ., data = Boston, distribution = "gaussian", n.trees = 20 , n.minobsinnode = 20/3)
summary(boost.boston)
my.boost=boost(data = Boston, resp = "medv", depth = 4, iter = 200, min.obs = 20, rate = 0.1)
my.boost$importance
boost.boston <- gbm(medv ∼ ., data = Boston, distribution = "gaussian", n.trees = 200 , n.minobsinnode = 20/3, bag.fraction = 1, interaction.depth = 3)
summary(boost.boston)
plot(my.boost$importance)
boost = function(data, resp, depth, iter, min.obs, rate, ...){
# depth = maximum of terminal nodes allowed. This is equal to the number of splits + 1
# iter = number of iterations
feat = names(data)[names(data)!=resp]
# for binary response:
if (is.factor(data[,resp])){
NN = nrow(data)
weight = rep(1/NN, nrow(data))
tree = classtree(data = data, min.obs = min.obs, resp = resp, depth = depth)
pred = predtree(newdata = data, resp = resp, res = tree$output, data = data)[, resp]
missclass = sum(pred != data[,resp])
error = sum(pred != data[,resp]) / NN
stageValue = log((1 - error) / error)
weight = weight * exp(ifelse(test = pred != data[,resp], -1, 1) * stageValue)
data = data[sample(1:NN, NN, replace = TRUE, prob = weight),]
}
# for continuous response:
if (!is.factor(data[,resp])){
pred_list = tree_list = importance_list = resid_list = list()
pred_list[[1]] = rep(mean(data[, resp]), nrow(data))
resid_list[[1]] = data[, resp] - pred_list[[1]]
for (i in 1:iter){
tree = regtree(data = data.frame(data[,feat], resid = resid_list[[i]]), min.obs = min.obs, resp = "resid", depth = depth)
tree_list[[i]] = tree$output
pred_resid = predtree(newdata = data, resp = resp, res = tree$output, data = data)[,resp]
pred_list[[i+1]] =  pred_list[[i]] + pred_resid*rate
resid_list[[i + 1]] = data[, resp] - pred_list[[i+1]]
importance_list[[i]] = tree$var_rank
}
importance = apply(do.call(cbind, importance_list), MARGIN = 1, mean)
return(list(pred = pred_list[[iter+1]], importance = data.frame(var = names(importance), influence = sort(importance, decreasing = TRUE)), trees = tree_list, resp = resp, data = data, rate = rate))
}
}
data.frame(var = names(my.boost$importance), influence = my.boost$importance)
my.boost$pred
boost.boston$fit
plot(my.boost$pred, boost.boston$fit)
abline(0,1)
plot(my.boost$pred, boost.boston$fit, xlab = "my predictions", ylab = "package")
abline(0,1)
boost.boston$train.error
boost.boston$c.splits
boost.boston$valid.error
boost.boston$bag.fraction
boost.boston$cv.folds
predict(boost.boston, newdata = Boston[1:3,])
nrow(Boston)
my.boost2=boost(data = Boston[1:450, ], resp = "medv", depth = 4, iter = 200, min.obs = 20, rate = 0.1)
boost_pred(newdata = Boston[451:506,], my.boost2)
gbmpred = predict(boost.boston, newdata = Boston[451:506,])
mypred = boost_pred(newdata = Boston[451:506,], my.boost2)
plot(mypred, gbmpred, xlab = "my predictions", ylab = "package")
abline(0,1)
my.boost2=boost(data = Boston[1:450, ], resp = "medv", depth = 4, iter = 200, min.obs = 20, rate = 0.1)
boost.boston2 <- gbm(medv ∼ ., data = Boston[1:450, ], distribution = "gaussian", n.trees = 200 , n.minobsinnode = 20/3, bag.fraction = 1, interaction.depth = 3)
gbmpred = predict(boost.boston, newdata = Boston[451:506,])
mypred = boost_pred(newdata = Boston[451:506,], my.boost2)
plot(mypred, Boston[451:506,medv], xlab = "my predictions", ylab = "package")
plot(mypred, Boston[451:506, "medv"], xlab = "my predictions", ylab = "package")
abline(0,1)
plot(gbmpred, Boston[451:506, "medv"], xlab = "my predictions", ylab = "package")
abline(0,1)
boost_pred = function(newdata, boost_object){
trees = boost_object$trees
resp = boost_object$resp
rate = boost_object$rate
iter = length(trees)
data = boost_object$data
pred_list = resid_list = list()
pred_list[[1]] = rep(mean(data[, resp]), nrow(newdata))
resid_list[[1]] = newdata[, resp] - pred_list[[1]]
for (i in 1:iter){
pred_resid = predtree(newdata = newdata, resp = resp, res = trees[[i]], data = data)[,resp]
pred_list[[i+1]] =  pred_list[[i]] + pred_resid*rate
resid_list[[i + 1]] = newdata[, resp] - pred_list[[i+1]]
}
return(pred = pred_list[[iter+1]])
}
mypred = boost_pred(newdata = Boston[451:506,], my.boost2)
plot(mypred, Boston[451:506, "medv"], xlab = "my predictions", ylab = "package")
abline(0,1)
plot(mypred, gbmpred, xlab = "my predictions", ylab = "package")
abline(0,1)
plot(mypred, Boston[451:506, "medv"], xlab = "my predictions", ylab = "package")
abline(0,1)
plot(gbmpred, Boston[451:506, "medv"], xlab = "my predictions", ylab = "package")
abline(0,1)
plot(mypred, gbmpred, xlab = "my predictions", ylab = "package")
abline(0,1)
mypred
gbmpred
plot(gbmpred, Boston[451:506, "medv"], xlab = "my predictions", ylab = "package")
abline(0,1)
plot(gbmpred, Boston[451:506, "medv"], xlab = "package", ylab = "actual data")
abline(0,1)
plot(mypred, Boston[451:506, "medv"], xlab = "my predictions", ylab = "actual data")
abline(0,1)
mean((gbmpred - Boston[451:506, "medv"])^2)
mean((mypred - Boston[451:506, "medv"])^2)
mypred
?gbm
