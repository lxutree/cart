---
title: "CART with RHC dataset"
author: "Liang"
date: "28/11/2021"
output: html_document
---

```{r}
setwd("~/../LX_Project/cart/rhc/")
ObsData <- read.csv("rhc.csv")
```

# Introduction

Tree-based methods are often easy to understand, interpret and visualize. Unlike regression models, tree-based methods does not make distributional assumptions about the outcome. One also does not need to specify interactions amongst covariates. Decision trees attempt at explaining variability in the outcome by organizing the predictors into distinct regions in a stepwise fashion. 

The right heart catherization (RHC) study (Connors et al., 1996) will be used as an example in this tutorial to understand how decision trees are built. RHC was believed to lead to better patient outcomes by many physician. Hence the study aimed to examine the association of right heart catheterization (RHC) during the first 24 hours of care in the intensive care unit and subsequent survival, length of stay, intensity of care, and cost of care. Data on 5735 critically ill patients being treated in an intensive care unit were collected.  We will use these data to illustrate how classification and regression trees (CARTs) work, focusing on length of stay as the outcome variable. 

Data are first cleaned following the steps outlined in "R Guide for TMLE in Medical Research" (https://ehsanx.github.io/TMLEworkshop/).

```{r}
# add column for outcome Y: length of stay 
# Y = date of discharge - study admission date
# Y = date of death - study admission date if date of discharge not available
ObsData$Y <- ObsData$dschdte - ObsData$sadmdte
ObsData$Y[is.na(ObsData$Y)] <- ObsData$dthdte[is.na(ObsData$Y)] - 
  ObsData$sadmdte[is.na(ObsData$Y)]
# remove outcomes we are not examining in this example
ObsData <- dplyr::select(ObsData, 
                         !c(dthdte, lstctdte, dschdte, death, t3d30, dth30, surv2md1))
# remove unnecessary and problematic variables 
ObsData <- dplyr::select(ObsData, 
                         !c(sadmdte, ptid, X, adld3p, urin1, cat2))

# convert all categorical variables to factors 
factors <- c("cat1", "ca", "cardiohx", "chfhx", "dementhx", "psychhx", 
             "chrpulhx", "renalhx", "liverhx", "gibledhx", "malighx", 
             "immunhx", "transhx", "amihx", "sex", "dnr1", "ninsclas", 
             "resp", "card", "neuro", "gastr", "renal", "meta", "hema", 
             "seps", "trauma", "ortho", "race", "income")
ObsData[factors] <- lapply(ObsData[factors], as.factor)
# convert our treatment A (RHC vs. No RHC) to a binary variable
ObsData$A <- ifelse(ObsData$swang1 == "RHC", 1, 0)
ObsData <- dplyr::select(ObsData, !swang1)
# Categorize the variables to match with the original paper
ObsData$age <- cut(ObsData$age,breaks=c(-Inf, 50, 60, 70, 80, Inf),right=FALSE)
ObsData$race <- factor(ObsData$race, levels=c("white","black","other"))
ObsData$sex <- as.factor(ObsData$sex)
ObsData$sex <- relevel(ObsData$sex, ref = "Male")
ObsData$cat1 <- as.factor(ObsData$cat1)
levels(ObsData$cat1) <- c("ARF","CHF","Other","Other","Other",
                          "Other","Other","MOSF","MOSF")
ObsData$ca <- as.factor(ObsData$ca)
levels(ObsData$ca) <- c("Metastatic","None","Localized (Yes)")
ObsData$ca <- factor(ObsData$ca, levels=c("None",
                                          "Localized (Yes)","Metastatic"))

# Rename variables
names(ObsData) <- c("Disease.category", "Cancer", "Cardiovascular", 
                    "Congestive.HF", "Dementia", "Psychiatric", "Pulmonary", 
                    "Renal", "Hepatic", "GI.Bleed", "Tumor", 
                    "Immunosupperssion", "Transfer.hx", "MI", "age", "sex", 
                    "edu", "DASIndex", "APACHE.score", "Glasgow.Coma.Score", 
                    "blood.pressure", "WBC", "Heart.rate", "Respiratory.rate", 
                    "Temperature", "PaO2vs.FIO2", "Albumin", "Hematocrit", 
                    "Bilirubin", "Creatinine", "Sodium", "Potassium", "PaCo2", 
                    "PH", "Weight", "DNR.status", "Insurance", 
                    "Respiratory.Diag", "Cardiovascular.Diag", 
                    "Neurological.Diag", "Gastrointestinal.Diag", "Renal.Diag",
                    "Metabolic.Diag", "Hematologic.Diag", "Sepsis.Diag", 
                    "Trauma.Diag", "Orthopedic.Diag", "race", "income", 
                    "Y", "A")

levels(ObsData$Insurance) = c("Medicaid", "Medicare", "M&M", "None", "Priv", "Priv&Medicare")

head(ObsData)
```

# Regression tree

### Outline of steps:

1. Set root of the tree as the full dataset

2. For each node that can be split further, determine the optimal split rule.

- for a classification tree, we can compute the gini index for each feature, then select the one with the lowest gini index. The gini index for a feature is calculated by substracting from 1 the sum of square proportions:
  
  $$ Gini_k = 1 - (p_k ^ 2 + (1 - p_k) ^ 2) $$
  
  , where $p_k$ is the proportion of the outcome for the $k^\text{th}$ split of the feature.
  
  Then the weighted sum of gini indices is computed:
  
  $$ \sum_k \frac{n_k}{N} Gini_k$$
  
  , where $n_k$ is the size of the $k^\text{th}$ split of the feature, and $N$ is the total sample size.
  
- for a regression tree, we can compute the SSE of each possible split for each feature, then choose the feature with the lowest SSE 

  $$ SSE = \sum_k \sum_i (Y_{ik} - \bar{Y}_k)^2$$
  
  , where $Y_{ik}$ is the $i^\text{th}$ observation of the $k^\text{th}$ split of the feature. 
  
3. Split the dataset accordingly.

4. Examine the children nodes - determine if they are ‘leaves’ or need to be split further. 
- For example, we consider a node terminal (i.e. a ‘leaf’) if it has less than a certain number of observations, and/or if it contains observations with the same label.

5. repeat 2-4 until each node is either ‘root’, ‘parent’, or ‘leaf’

### Breakdown of each step with R codes:

1. Set root of the tree as the full dataset
```{r}
  resp = "Y"; data = ObsData
  output = data.frame(status = "split", count = nrow(data), "split rule" = "root", iter = 0, mean = mean(data[, resp]), stringsAsFactors = FALSE)
```


2. For each node that can be split further, determine the optimal split rule.

- First identity which nodes are labeled "split". As of now there is only the root:
```{r}
which(output$status == "split")
```

- Now we compute the SSE for each covariate. To demonstrate the process, let's find the SSE for the covariate 'Cardiovascular' by hand:

```{r}
data_sub = data.frame(var = data[, 'Cardiovascular'], resp = data[, resp])
data.split = split(data_sub, data_sub$var)

# sum of squares for 'Cardiovascular' = 0 and 'Cardiovascular' = 1:
sum0 = sum( (data.split$`0`$resp - mean(data.split$`0`$resp)) ^ 2 )
sum1 = sum( (data.split$`1`$resp - mean(data.split$`0`$resp)) ^ 2 )

# adding those together:
sum0 + sum1
```

- Now repeat the same process for remaining covariates:
```{r}
feat = names(data)[names(data)!=resp]
error = c() # to record each feature's SSE
split_val = list() # to record the most optimal split  for categorical and continuous features

 for (i in 1:length(feat)){
        data_sub = data.frame(var = data[, feat[i]], resp = data[, resp])
        
        # if feature is a factor:
        if( is.factor(data_sub$var) ) {
          
          # if factor has more than 2 levels (categorical)
          if( length(levels(data_sub$var)) > 2 ){
            
            # find all possible binary splits
            kk = 1
            varcomb = list()
            for (ii in 1:length(levels(data_sub$var))) {
              comb = combn(length(levels(data_sub$var)), ii)
              for (jj in 1:ncol(comb)){
                varcomb[[kk]] = levels(data_sub$var)[comb[, jj]]
                kk = kk +1
              }
            }
            
            # calculate sse for all possible splits
            sse = sapply(varcomb, function(varcomb_i){ 
              sum((data_sub$resp[data_sub$var %in% varcomb_i] - mean(data_sub$resp[data_sub$var %in% varcomb_i]))^2) + sum((data_sub$resp[!(data_sub$var %in% varcomb_i)] - mean(data_sub$resp[!(data_sub$var %in% varcomb_i)]))^2) })
            
            # record the split with minimal sse
            error[i] = min(sse, na.rm = TRUE)
            split_val[[i]] = varcomb[[which.min(sse)]]
            
          } else {
            data.split = split(data_sub, data_sub$var)
            error[i] = sum(sapply(data.split, function(x){
              sum( (x$resp - mean(x$resp)) ^ 2 )
            })) 
            
          }
        } else {
        # calculating sse for continuous feature:
          splits_sort = sort(unique(data_sub$var)) # all possible splits
          sse <- c() 
          
          # calculating sse for all possible splits
          for( k in 1:length(splits_sort)){
            sse[k] = sum( (data_sub$resp[data_sub$var < splits_sort[k]] - mean(data_sub$resp[data_sub$var < splits_sort[k]]) )^2 ) + sum( (data_sub$resp[data_sub$var >= splits_sort[k]] - mean(data_sub$resp[data_sub$var >= splits_sort[k]]) )^2 ) 
            
          }
          
          # clean up for when none of the splits is valid:
          error[i] = min(sse, na.rm = TRUE)
          split_val[[i]] = splits_sort[which.min(sse)]
        }
      }
error
```

- Find the feature that leads to the smallest SSE, which is 'Disease.category'. The split rule tells us that the data should be split according to whether the disease category is one of "ARF" and "MOSF" or not. 
```{r}
feat[which.min(error)]
split_val[which.min(error)]
```

3. Split the dataset accordingly.

The first children node contains all those entries with disease category "ARF" or "MOSF". The other node contains all those entries with a disease category that is neither "ARF" nor "MOSF".
```{r}
children = list()
children[[1]] = data[(data[["Disease.category"]] %in% c("ARF", "MOSF")), ]
children[[2]] = data[!(data[["Disease.category"]] %in% c("ARF", "MOSF")), ]
```

4. Examine the children nodes - determine if they are ‘leaves’ or need to be split further. 

- Let's set the minimum number of observations in each parent node to be 500
- In addition, we check if the all observations have the same label

Both children nodes should be split further:

```{r}
min.obs = 500 

if (nrow(children[[1]]) < min.obs | nrow( unique(children[[1]][, -which(names(children[[1]]) %in% resp)]) ) == 1) status1 = "leaf" else status1 = "split" 
          
status1

if (nrow(children[[1]]) < min.obs | nrow( unique(children[[1]][, -which(names(children[[1]]) %in% resp)]) ) == 1) status2 = "leaf" else status2 = "split" 
          
status2
```

5. repeat 2-4 until each node is either ‘root’, ‘parent’, or ‘leaf’

To automate the process of branching and growing the tree, we make use of the while loop, which keeps running until there are no more nodes with status "split". To make the results comparable with functions in the package 'rpart', we further restrict the size of terminal nodes (leaf size) to be at least a third of the minimum size for parent nodes (500). 

```{r}
data.list = list(); data.list[[1]] = data

leafsize = min.obs/3

stopsplit=FALSE

# empty vectors for variable importance
feat_vec = c()
diff_vec = c()
  
while(!stopsplit) { # while loop will keep going as long as stopsplit is 'FALSE'
    
    # list of splits to be done:
    split.queue = which(output$status == "split")
    
    for (j in split.queue[1]) {
      
      # empty objects for later use 
      weighted.sd = c()
      error = c()
      split_val = list()
      
      # load data corresponding to current node
      data.temp = data.list[[j]]
      
      for (i in 1:length(feat)){ # loop through all features
        data_sub = data.frame(var = data.temp[, feat[i]], resp = data.temp[, resp])
        
        # sse of parent node
        rss_parent = sum((data_sub$resp - mean(data_sub$resp))^2)
        
        # calculating sse for categorical feature:
        if( is.factor(data_sub$var) ) {
          
          if( length(levels(data_sub$var)) > 2 ){
            # find all possible binary splits
            kk = 1
            varcomb = list()
            for (ii in 1:(length(levels(data_sub$var))-1)) {
              comb = combn(length(levels(data_sub$var)), ii)
              for (jj in 1:ncol(comb)){
                varcomb[[kk]] = levels(data_sub$var)[comb[, jj]]
                kk = kk +1
              }
            }
            
            # calculate sse for all possible splits
            sse = sapply(varcomb, function(varcomb_i){ 
              sum((data_sub$resp[data_sub$var %in% varcomb_i] - mean(data_sub$resp[data_sub$var %in% varcomb_i]))^2) + sum((data_sub$resp[!(data_sub$var %in% varcomb_i)] - mean(data_sub$resp[!(data_sub$var %in% varcomb_i)]))^2) })
            
            # checking size of children nodes; if less than specified, the split is not considered
            count_min = sapply(varcomb, function(varcomb_i){min(length(data_sub$resp[data_sub$var %in% varcomb_i]), length(data_sub$resp[!(data_sub$var %in% varcomb_i)]))})
            for(ii in 1:length(varcomb)) {if(count_min[ii] < round(leafsize)) sse[ii] = NA}
            
            # clean up:
            if(all(is.na(sse))) {error[i] = NA; split_val[[i]] = NA} else { error[i] = min(sse, na.rm = TRUE); split_val[[i]] = varcomb[[which.min(sse)]]}
            
          } else {
            data.split = split(data_sub, data_sub$var)
            error[i] = sum(sapply(data.split, function(x){
              sum( (x$resp - mean(x$resp)) ^ 2 )
            })) 
            
            # checking size of children nodes; if less than specified, the split is not considered
            count_min = min(sapply(data.split, nrow))
            if( count_min < leafsize) error[i] = NA
          }
        } else {
        # calculating sse for continuous feature:
          splits_sort = sort(unique(data_sub$var)) # all possible splits
          sse <- c() 
          
          # calculating sse for all possible splits
          for( k in 1:length(splits_sort)){
            sse[k] = sum( (data_sub$resp[data_sub$var < splits_sort[k]] - mean(data_sub$resp[data_sub$var < splits_sort[k]]) )^2 ) + sum( (data_sub$resp[data_sub$var >= splits_sort[k]] - mean(data_sub$resp[data_sub$var >= splits_sort[k]]) )^2 ) 
            
            # checking size of children nodes; if less than specified, the split is not considered
            count_min = min(length(data_sub$resp[data_sub$var < splits_sort[k]]), length(data_sub$resp[data_sub$var >= splits_sort[k]]))
            if(count_min < round(leafsize)) sse[k] = NA
          }
          # clean up for when none of the splits is valid:
          if(all(is.na(sse))) {error[i] = NA; split_val[[i]] = NA} else { error[i] = min(sse, na.rm = TRUE); split_val[[i]] = splits_sort[which.min(sse)]}
        }
      }
      
      if(all(is.na(error))){
        # if none of the splits is good, consider the current node to 'leaf'
        output$status[j] = "leaf"
      } else {
        # characteristics of the current split
        splitvar = feat[which.min(error)] # feature leading to the lowest sse
        rss_diff = rss_parent - min(error, na.rm = TRUE) # difference in sse
        feat_vec = c(feat_vec, splitvar) # record feature used to split
        diff_vec = c(diff_vec, rss_diff) # record different in sse
        
        # creating children nodes:
        if( is.factor(data.temp[[splitvar]])) {
          # for categorical feature:
          
          if( length(levels(data.temp[[splitvar]])) > 2 ){
            yeslevels = split_val[[which.min(error)]]
            nolevels = levels(data.temp[[splitvar]])[!(levels(data.temp[[splitvar]]) %in% yeslevels)]
            data.next = list()
            data.next[[1]] = data.temp[data.temp[[splitvar]] %in% nolevels, ]
            data.next[[2]] = data.temp[data.temp[[splitvar]] %in% yeslevels, ]
          } else data.next = split(data.temp, data.temp[ , splitvar])
        } else {
          # for continuous feature:
            value = split_val[[which.min(error)]]
            index = which(sort(unique(data.temp[[splitvar]])) == value)
            # taking the middle point of unique values as the splitting point to be consistent with 'rpart':
            value = (sort(unique(data.temp[[splitvar]]))[index] + sort(unique(data.temp[[splitvar]]))[index-1])/2
            data.next = list()
            data.next[[1]] = data.temp[which(data.temp[[splitvar]] <= value), ]
            data.next[[2]] = data.temp[which(data.temp[[splitvar]] > value), ]
        }
        
  
        # Stopping criteria: 
        # - less than 3 observations
        # - all observations have the same label
        status = sapply(data.next, function(x){
          if (ncol(x) > 2) {
            if (nrow(x) < min.obs | nrow( unique(x[, -which(names(x) %in% resp)]) ) == 1) status = "leaf" else status = "split" 
          } else status = "leaf"
          
          status
        })
        
        # change current status from 'split' to 'parent' so it won't be split further:
        output$status[j] = "parent"
        
        # record how the split was done:
        if( is.factor(data.temp[[splitvar]]) ) {
          if( length(levels(data.temp[[splitvar]])) > 2 ){
            splitrule = c(paste(splitvar, "=", paste(nolevels, collapse = ",")), paste(splitvar, "=", paste(yeslevels, collapse = ",")) )
          } else splitrule = sapply(names(data.next), function(x){paste(splitvar, "=" , x)})
        } else {
          splitrule = c(paste(splitvar, "<=", value),paste(splitvar, ">", value) )
        }
  
        # creating outputs
        temp.output = data.frame(status = status, count = sapply(data.next, nrow), "split rule" = splitrule, iter = output$iter[j] + 1, row.names = NULL, mean = sapply(data.next, function(x){mean(x[[resp]])}))
        
        # attach new outputs to existing dataframe
        output = rbind(output[1:j,], temp.output, output[-c(1:j), ])
        names(data.next) = NULL; data.list = c(data.list[1:j], data.next, data.list[-c(1:j)])
      
      # check if there are remaining splits to be done:
      if(all(output$status != "split")) stopsplit = TRUE
    }
    }
    
  # summing up RSS difference for each feature 
  allfeat = names(data)[names(data)!=resp]
  rss_sum = c()
  for (i in 1:length(allfeat)){
    rss_sum[i] = sum(diff_vec[which(feat_vec == allfeat[i])])
  }
  var_rank = data.frame(var.names = allfeat, criterion = rss_sum)
}

output
```

To gauge the relative importance of the covariates, we can sum up the 'total contribution' for each covariate. For regression trees, we can take the sum of reductions in SSE across all splits a covariate is involved with: 

```{r}
var_rank[order(var_rank$criterion, decreasing = TRUE),]
```

The tree built by 'rpart' is as follows:
```{r}
library(rpart)
rpart_object = rpart(Y ~., data = ObsData, minsplit = 500, cp = -1)
```

Variable importance according to 'rpart':
```{r}
rpart_object$variable.importance
```

