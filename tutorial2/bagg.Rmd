---
title: "bagg"
author: "Liang"
date: "07/01/2022"
output: html_document
---

```{r}
library(stringr)
library(rpart)

setwd("~/../LX_Project/cart/tutorial2//")
```

### Introduction

Previously we explored two basic tree-based models: classification and regression trees. The performance of a single regression or classification tree is limited. Ensemble methods combine a number of simpler models and are more powerful than individual decision trees. This tutorial will help the readers familiarize with popular ensemble techniques including bootstrap aggregating (bagging), random forest and boosting. The right heart catheterization (RHC) and diabetes readmission datasets will be used to construct examples. 

#### Bootstrap Aggregating

Predictions from individual decision trees tend to have high variance. Different subsamples could lead to different tree structures and, as a result, unstable predictions. Bootstrap aggregating (bagging) reduces variance by aggregating the predictions of trees built from bootstrap samples. A bootstrap sample is generated by sampling with replacement from the original dataset. For regression trees, the average of predictions is taken as the aggregated prediction. For classification trees, the aggregated prediction corresponds to the category with the highest frequency of appearance (majority vote) amongst the predictions.

Bagging also provides an internal assessment of the model's predictive ability. For a bootstrap sample, some observations are left out (out-of-bag). Since the out-of-bag observations are not involved in generating the predictions for the bootstrap sample, they can be used to compute the out-of-bag error as a measure of the bagging model's predictive performance. 

##### Data cleaning

The RHC dataset is prepared following the steps outlined in "R Guide for TMLE in Medical Research" (https://ehsanx.github.io/TMLEworkshop/).

```{r}
rhcdata <- read.csv("rhc.csv") # load data

# add column for outcome Y: length of stay 
# Y = date of discharge - study admission date
# Y = date of death - study admission date if date of discharge not available
rhcdata$Y <- rhcdata$dschdte - rhcdata$sadmdte
rhcdata$Y[is.na(rhcdata$Y)] <- rhcdata$dthdte[is.na(rhcdata$Y)] - 
  rhcdata$sadmdte[is.na(rhcdata$Y)]
# remove outcomes we are not examining in this example
rhcdata <- dplyr::select(rhcdata, 
                         !c(dthdte, lstctdte, dschdte, death, t3d30, dth30, surv2md1))
# remove unnecessary and problematic variables 
rhcdata <- dplyr::select(rhcdata, 
                         !c(sadmdte, ptid, X, adld3p, urin1, cat2))

# convert all categorical variables to factors 
factors <- c("cat1", "ca", "cardiohx", "chfhx", "dementhx", "psychhx", 
             "chrpulhx", "renalhx", "liverhx", "gibledhx", "malighx", 
             "immunhx", "transhx", "amihx", "sex", "dnr1", "ninsclas", 
             "resp", "card", "neuro", "gastr", "renal", "meta", "hema", 
             "seps", "trauma", "ortho", "race", "income")
rhcdata[factors] <- lapply(rhcdata[factors], as.factor)
# convert our treatment A (RHC vs. No RHC) to a binary variable
rhcdata$A <- ifelse(rhcdata$swang1 == "RHC", 1, 0)
rhcdata <- dplyr::select(rhcdata, !swang1)
# Categorize the variables to match with the original paper
rhcdata$age <- cut(rhcdata$age,breaks=c(-Inf, 50, 60, 70, 80, Inf),right=FALSE)
rhcdata$race <- factor(rhcdata$race, levels=c("white","black","other"))
rhcdata$sex <- as.factor(rhcdata$sex)
rhcdata$sex <- relevel(rhcdata$sex, ref = "Male")
rhcdata$cat1 <- as.factor(rhcdata$cat1)
levels(rhcdata$cat1) <- c("ARF","CHF","Other","Other","Other",
                          "Other","Other","MOSF","MOSF")
rhcdata$ca <- as.factor(rhcdata$ca)
levels(rhcdata$ca) <- c("Metastatic","None","Localized (Yes)")
rhcdata$ca <- factor(rhcdata$ca, levels=c("None",
                                          "Localized (Yes)","Metastatic"))

# Rename variables
names(rhcdata) <- c("Disease.category", "Cancer", "Cardiovascular", 
                    "Congestive.HF", "Dementia", "Psychiatric", "Pulmonary", 
                    "Renal", "Hepatic", "GI.Bleed", "Tumor", 
                    "Immunosupperssion", "Transfer.hx", "MI", "age", "sex", 
                    "edu", "DASIndex", "APACHE.score", "Glasgow.Coma.Score", 
                    "blood.pressure", "WBC", "Heart.rate", "Respiratory.rate", 
                    "Temperature", "PaO2vs.FIO2", "Albumin", "Hematocrit", 
                    "Bilirubin", "Creatinine", "Sodium", "Potassium", "PaCo2", 
                    "PH", "Weight", "DNR.status", "Insurance", 
                    "Respiratory.Diag", "Cardiovascular.Diag", 
                    "Neurological.Diag", "Gastrointestinal.Diag", "Renal.Diag",
                    "Metabolic.Diag", "Hematologic.Diag", "Sepsis.Diag", 
                    "Trauma.Diag", "Orthopedic.Diag", "race", "income", 
                    "Y", "A")

levels(rhcdata$Insurance) = c("Medicaid", "Medicare", "M&M", "None", "Priv", "Priv&Medicare")

rhcdata = rhcdata[1:100,] # temporary for compiling speed
```

##### Step by step breakdown

1. Set $K$ as the number of bootstrap samples and $N$ as the total number of observations. In the case of the RHC data, $N = 5735$. Let $K = 100$.

```{r}
nrow(rhcdata)
```

2. For each bootstrap sample $i$ from $1$ to $K$, sample $N$ observations randomly with replacement to generate the $i$^th^ bootstrap sample. 

```{r}
data_list = index = list() # empty list to store bootstrap samples
for(i in 1:100){
  index[[i]] = sample(nrow(rhcdata),  replace = TRUE)
  data_list[[i]] = rhcdata[index[[i]], ]
}
```

3. For each bootstrap sample $i$ from $1$ to $K$, build a regression tree.

We will use package 'rpart' to generate the trees. Let the minimum node side be 1000.

```{r}
library(rpart)
tree_list = list() # empty list to store trees
for(i in 1:100){
  tree_list[[i]] = rpart(formula = Y ~ ., minsplit = 1000, data = data_list[[i]], cp=-1)
}
```

4. For each observation in the original dataset $j$ from $1$ to $N$, generate a prediction using the set of bootstrap models that did not include it. Different bootstrap models can generate different predictions for the same observation. Such predictions can be combined to yield a single estimate. For continuous response, the estimate can be the average of the predictions. For binary response, the estimate can be the majarity vote, which corresponds to the category with the highest frequency of appearance.   

```{r}
pred = c() # vector of predictions

# for the ith observation:
for(i in 1:nrow(rhcdata)){
  # the ith prediction is calculated from trees that did not use the ith observation :
  pred_i = sapply(1:length(index), function(j){
    # only produce a prediction from the jth model if it didn't use the ith observation
    if((i %in% index[[j]]) == FALSE) predict(tree_list[[j]], newdata = rhcdata[i,]) else NA
       })
  # record the mean of predictions
  pred[i] = mean(pred_i, na.rm = TRUE)
}
```

5. Compute the out-of-bag error

The out-of-bag error for continuous response is given by the root of the mean squared difference between the predictions and the oberved values:

```{r}
sqrt(mean((pred - rhcdata$Y)^2))
```

The out-of-bag error for binary response is the rate of missclassification.

##### Comparison with existing software

Compare the results above against outputs from an existing R package 'ipred'. The out-of-bag error given by 'bagging' should be similar to the out-of-bad error above.

```{r}
library(ipred)
bagging(
  formula = Y ~ .,
  data = rhcdata,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 1000, cp = -1)
)$err

```

##### <Second example with binary response (e.g. diabetes dataset)>

#### Random Forest

In bagging, bootstrap samples are generated to build trees that vary in structure for better predictive performance. Despite the randomness introduced by bootstrapping, the resulting trees and their predictions are correlated, as the same set of predictors are used to build all trees. Random forest reduces correlation of the trees by selecting a random subset of predictors for each split. This introduces additional randomness that further reduces variance in the prediction. A good starting point for the number of randomly chosen predictors at each split is $\sqrt{p}$, where $p$ is the total number of predictors. Similar to bagging, random forest combines all the trees to generate an aggregated prediction.

#### Boosting

Unlike in bagging and random forest where trees are built independently of each other, trees are built sequentially in boosting. Boosting makes use of small trees that learn slowly to avoid overfitting. The size of the tree can be controlled by the number of terminal nodes (or the number of splits plus one), one of the tuning parameters of the algorithm. A tree is built using the residuals in the previous step rather than the outcome itself and are used to improve the previous model. The rate at which the model improves is controlled by the shrinkage parameter, which is typically very small (e.g. 0.01 or less). 

Improvement in model performance decreases exponentially with the number of bootstrap samples.

Ensemble models are more powerful but also more difficult to interpret. But can still quantify the relative importance of predictors. 